---
# Kube Prometheus Stack Helm Values Ref: https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack
serviceMonitorSelectorNilUsesHelmValues: false
defaultRules:
  create: true
prometheusOperator:
  verticalPodAutoscaler:
    enabled: true
  # Based on: https://grafana.needham.wsbidev.net/d/a164a7f0339f99e89cea5cb47e9be617/kubernetes-compute-resources-workload?var-datasource=default&var-cluster=&var-namespace=kube-prometheus-stack&var-workload=kube-prometheus-stack-operator&var-type=All&orgId=1&refresh=10s&from=now-30d&to=now
  resources:
    limits:
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 64Mi
      ephemeral-storage: 128Mi
prometheus:
  prometheusSpec:
    podMonitorNamespaceSelector:
      any: true
    podMonitorSelectorNilUsesHelmValues: false
    ruleNamespaceSelector:
      any: true
    ruleSelectorNilUsesHelmValues: false
    serviceMonitorNamespaceSelector:
      any: true
    serviceMonitorSelectorNilUsesHelmValues: false
    probeNamespaceSelector:
      any: true
    probeSelectorNilUsesHelmValues: false
    scrapeConfigNamespaceSelector:
      any: true
    scrapeConfigSelectorNilUsesHelmValues: false
    # Based on: https://grafana.needham.wsbidev.net/d/a164a7f0339f99e89cea5cb47e9be617/kubernetes-compute-resources-workload?var-datasource=default&var-cluster=&var-namespace=kube-prometheus-stack&var-workload=prometheus-kube-prometheus-stack-prometheus&var-type=All&orgId=1&refresh=10s&from=now-30d&to=now
    resources:
      limits:
        memory: 4Gi
      requests:
        cpu: 400m
        memory: 2560Mi
        ephemeral-storage: 128Mi
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: ceph-block
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
# Config Ref: https://artifacthub.io/packages/helm/grafana/grafana
grafana:
  # To use grafana.com dashboards:
  # ref: https://github.com/prometheus-community/helm-charts/issues/2477#issuecomment-1276224026
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: provider-site
          orgId: 1
          folder: ""
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/provider-site
  # now using that provider to import dashboards
  dashboards:
    provider-site:
      # https://grafana.com/grafana/dashboards/19974-argocd-application-overview/?tab=revisions
      argocd-application-overview:
        gnetId: 19974
        revision: 1
        datasource: Prometheus
      # https://grafana.com/grafana/dashboards/19993-argocd-operational-overview/?tab=revisions
      argocd-operational-overview:
        gnetId: 19993
        revision: 1
        datasource: Prometheus
      # https://grafana.com/grafana/dashboards/19975-argocd-notifications-overview/?tab=revisions
      argocd-notifications-overview:
        gnetId: 19975
        revision: 1
        datasource: Prometheus
      # https://grafana.com/grafana/dashboards/2842-ceph-cluster/?tab=revisions
      ceph-cluster:
        gnetId: 2842
        revision: 17
        datasource: Prometheus
      # https://grafana.com/grafana/dashboards/5336-ceph-osd-single/?tab=revisions
      ceph-osd-single:
        gnetId: 5336
        revision: 9
        datasource: Prometheus
      # https://grafana.com/grafana/dashboards/5342-ceph-pools/?tab=revisions
      ceph-pools:
        gnetId: 5342
        revision: 9
        datasource: Prometheus
  replicas: 1
  autoscaling:
    # TODO: Grafana multi-replica doesn't currently work with persistence
    enabled: false
  persistence:
    enabled: true
    type: statefulset
    storageClassName: ceph-block
  grafana.ini:
    server:
      domain: grafana.needham.wsbidev.net
      root_url: https://grafana.needham.wsbidev.net
  ingress:
    enabled: true
    annotations:
      cert-manager.io/cluster-issuer: zerossl-prod
    ingressClassName: cilium
    tls:
      - hosts:
          - grafana.needham.wsbidev.net
        secretName: grafana-needham-wsbidev-net
    hosts:
      - grafana.needham.wsbidev.net
  # Based on: https://grafana.needham.wsbidev.net/d/a164a7f0339f99e89cea5cb47e9be617/kubernetes-compute-resources-workload?var-datasource=default&var-cluster=&var-namespace=kube-prometheus-stack&var-workload=kube-prometheus-stack-grafana&var-type=All&orgId=1&refresh=10s&from=now-30d&to=now
  resources:
    limits:
      memory: 1Gi
    requests:
      cpu: 20m
      memory: 256Mi
      ephemeral-storage: 128Mi
  downloadDashboards:
    resources:
      limits:
        memory: 1Gi
      requests:
        cpu: 50m
        memory: 50Mi
        ephemeral-storage: 128Mi
  sidecar:
    resources:
      limits:
        memory: 1Gi
      requests:
        cpu: 50m
        memory: 50Mi
        ephemeral-storage: 128Mi
  initChownData:
    resources:
      limits:
        memory: 1Gi
      requests:
        cpu: 100m
        memory: 128Mi
        ephemeral-storage: 128Mi
  imageRenderer:
    resources:
      limits:
        memory: 1Gi
      requests:
        cpu: 50m
        memory: 50Mi
        ephemeral-storage: 128Mi
# Config Ref: https://artifacthub.io/packages/helm/prometheus-community/prometheus-node-exporter
prometheus-node-exporter:
  # Based on: https://grafana.needham.wsbidev.net/d/a164a7f0339f99e89cea5cb47e9be617/kubernetes-compute-resources-workload?var-datasource=default&var-cluster=&var-namespace=kube-prometheus-stack&var-workload=kube-prometheus-stack-prometheus-node-exporter&var-type=All&orgId=1&refresh=10s&from=now-30d&to=now
  resources:
    limits:
      memory: 128Mi
    requests:
      cpu: 300m
      memory: 64Mi
      ephemeral-storage: 128Mi
# Config Ref: https://artifacthub.io/packages/helm/prometheus-community/kube-state-metrics
kube-state-metrics:
  verticalPodAutoscaler:
    # Our VPA uses metrics from kube-state-metrics to determine the new values for requests & limits.
    # DevZone Architecture disables the VPA object only for kube-state-metrics to avoid dependency loops.
    enabled: false
  # Based on: https://grafana.needham.wsbidev.net/d/a164a7f0339f99e89cea5cb47e9be617/kubernetes-compute-resources-workload?var-datasource=default&var-cluster=&var-namespace=kube-prometheus-stack&var-workload=kube-prometheus-stack-kube-state-metrics&var-type=All&orgId=1&refresh=10s&from=now-30d&to=now
  resources:
    limits:
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi
      ephemeral-storage: 128Mi
